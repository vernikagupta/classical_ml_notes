# PART 1: Linear Models & Geometry Foundations

This section builds the foundational concepts needed for all of classical ML.

## Contents

- **01-linear-regression/** - Vectors, projections, least squares, bias-variance tradeoff
- - **02-logistic-regression/** - Extending linear geometry to probabilities, likelihood, cross-entropy
 
  - ## Why This Order?
 
  - Linear Regression introduces the geometric intuitions (projections, loss surfaces) that underlie all subsequent models. Logistic Regression extends these intuitions to probabilistic settings, bridging deterministic and probabilistic approaches.
 
  - ## Key Concepts
 
  - - Vector spaces and projections
    - - Loss surfaces and optimization
      - - Least squares geometry
        - - Bias-variance decomposition
          - - Likelihood and cross-entropy
            - - Convex optimization basics
