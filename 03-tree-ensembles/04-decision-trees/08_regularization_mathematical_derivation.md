# Regularization ‚Äî Mathematical Derivation (Ridge, Lasso, ElasticNet)

This chapter derives **why regularization works mathematically**. We will start from Linear Regression and see exactly **what changes in the equations**, **why solutions become stable**, and **why some penalties create sparsity**.

> Read this as a story: problem ‚Üí math ‚Üí consequence.

---

## Q1. What problem appears in the OLS normal equation?

**What:**  
The OLS solution is:
Œ≤ÃÇ = (X·µÄX)‚Åª¬π X·µÄy

**Why this fails:**  
- X·µÄX can be singular or ill-conditioned
- Small eigenvalues ‚Üí huge variance in Œ≤ÃÇ

**Consequence:**  
Coefficients become unstable.

---

## Q2. How does Ridge Regression modify the optimization problem?

**What:**  
Ridge adds an L2 penalty to OLS.

**How (math):**  
Minimize:
||y ‚àí XŒ≤||¬≤ + Œª||Œ≤||¬≤

**Why:**  
To discourage large coefficients.

---

## Q3. How does Ridge change the normal equation?

**Derivation:**  
Take gradient and set to zero:

X·µÄXŒ≤ + ŒªŒ≤ = X·µÄy

**Solution:**  
Œ≤ÃÇ_ridge = (X·µÄX + ŒªI)‚Åª¬π X·µÄy

---

## Q4. Why does adding ŒªI fix singularity?

**Why:**  
X·µÄX + ŒªI shifts all eigenvalues by Œª.

**Interpretation:**  
Flat directions gain curvature.

**Result:**  
Matrix becomes invertible.

---

## Q5. How does Ridge reduce variance mathematically?

**Eigenvalue view:**  
Small eigenvalues are inflated by Œª.

**Effect:**  
Coefficient variance is reduced in unstable directions.

---

## Q6. Why does Ridge not produce exact zeros?

**Why:**  
The L2 penalty is smooth and differentiable.

**Consequence:**  
Weights are shrunk but rarely hit zero.

---

## Q7. What happens to Ridge solution as Œª ‚Üí 0?

**Result:**  
Ridge ‚Üí OLS.

---

## Q8. What happens as Œª ‚Üí ‚àû?

**Result:**  
Œ≤ÃÇ ‚Üí 0.

**Interpretation:**  
Model ignores features.

---

## Q9. Why does Lasso have no closed-form solution?

**What:**  
Lasso uses L1 penalty:
Œª Œ£ |Œ≤·µ¢|

**Why:**  
Absolute value is not differentiable at zero.

---

## Q10. How is Lasso optimized then?

**Methods:**  
- Coordinate descent
- Subgradient methods

**Key idea:**  
Optimize one coefficient at a time.

---

## Q11. Why does Lasso drive coefficients exactly to zero?

**Math intuition:**  
The subgradient allows zero to be an optimal point.

**Effect:**  
Automatic feature selection.

---

## Q12. What is the bias‚Äìvariance tradeoff for Lasso?

**Bias:**  
Higher than Ridge.

**Variance:**  
Lower when irrelevant features exist.

---

## Q13. When does Lasso become unstable?

**Why:**  
With highly correlated features.

**Effect:**  
Randomly selects one feature and drops others.

---

## Q14. How does ElasticNet combine L1 and L2?

**What:**  
ElasticNet penalty:
Œª‚ÇÅ||Œ≤||‚ÇÅ + Œª‚ÇÇ||Œ≤||¬≤

**Why:**  
- L1 ‚Üí sparsity
- L2 ‚Üí stability

---

## Q15. Why does ElasticNet work better with correlated features?

**Why:**  
L2 term encourages grouped selection.

**Interpretation:**  
Correlated features survive together.

---

## Q16. How does regularization affect gradient descent?

**Effect:**  
Adds an extra gradient term:

‚àÇ/‚àÇŒ≤ (Œª||Œ≤||¬≤) = 2ŒªŒ≤

**Result:**  
Weights are continuously pulled toward zero.

---

## Q17. Is regularization equivalent to adding noise?

**Yes (conceptually):**  
Ridge ‚âà Gaussian noise in inputs.

**Why:**  
Noise discourages reliance on exact values.

---

## Q18. What is the Bayesian interpretation of Ridge?

**View:**  
Ridge = MAP estimation with Gaussian prior on Œ≤.

**Interpretation:**  
We believe coefficients should be small.

---

## Q19. What is the Bayesian view of Lasso?

**View:**  
Lasso = MAP with Laplace prior.

**Effect:**  
Sharp peak at zero ‚Üí sparsity.

---

## Q20. Mathematical takeaway

- Ridge stabilizes solutions
- Lasso selects features
- ElasticNet balances both

All three modify the **geometry of the solution space**, not the data.

---

üìå **Next:** Geometric interpretation ‚Äî circles, diamonds, corners, and why sparsity emerges.