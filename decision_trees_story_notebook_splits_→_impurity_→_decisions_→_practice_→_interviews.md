# ðŸ“˜ DECISION TREES â€” A STORY OF QUESTIONS, CHOICES & PURITY

This notebook is written as a **story of decision-making**.

Read it like this:
> *A decision tree does not learn equations.  
It learns how to ask the **right questions in the right order**.*

---

## 1ï¸âƒ£ WHY DECISION TREES EXIST (THE HUMAN ANALOGY)

Humans reason like this:
- If age > 40, then check BP
- If BP is high, then check cholesterol

We do **conditional questioning**.

Decision trees formalize this logic.

They:
- Split data
- Reduce uncertainty step by step
- Stop when decisions become clear

---

## 2ï¸âƒ£ WHAT A DECISION TREE IS ACTUALLY DOING

At every node, the tree asks:
> *Which question should I ask now so that my uncertainty reduces the most?*

This uncertainty is called **impurity**.

---

## 3ï¸âƒ£ WHAT IS IMPURITY (INTUITION FIRST)

- A pure node â†’ all samples belong to one class
- An impure node â†’ mixed classes

50â€“50 split = maximum confusion.

---

## 4ï¸âƒ£ ENTROPY â€” UNCERTAINTY AS CONFUSION

### Intuition

Entropy measures:
> *How unsure am I before making a decision?*

If outcomes are equally likely, uncertainty is maximum.

---

### Formula

Entropy = âˆ’ Î£ páµ¢ logâ‚‚ páµ¢

Where páµ¢ is class probability.

---

### Example (step-by-step)

Dataset:
- 10 samples
- 6 Yes, 4 No

p(Yes)=0.6, p(No)=0.4

Entropy = âˆ’[0.6 logâ‚‚ 0.6 + 0.4 logâ‚‚ 0.4]

= âˆ’[0.6(âˆ’0.737) + 0.4(âˆ’1.322)]

= 0.97 (high uncertainty)

---

## 5ï¸âƒ£ GINI IMPURITY â€” MISCLASSIFICATION VIEW

### Intuition

Imagine:
- You randomly pick a data point
- You randomly assign a class according to node distribution

Gini = probability of **misclassification**.

---

### How the formula comes

Probability of correct classification:

Î£ páµ¢Â²

So probability of misclassification:

Gini = 1 âˆ’ Î£ páµ¢Â²

---

### Example

p(Yes)=0.9, p(No)=0.1

Correct classification = 0.9Â² + 0.1Â² = 0.82

Gini = 1 âˆ’ 0.82 = 0.18

Low impurity.

---

## 6ï¸âƒ£ INFORMATION GAIN â€” HOW SPLITS ARE CHOSEN

Information Gain =

Entropy(parent) âˆ’ weighted entropy(children)

Tree chooses split with **maximum gain**.

---

## 7ï¸âƒ£ FULL DATASET EXAMPLE (STEP-BY-STEP)

Dataset (Play Tennis):

| Outlook | Play |
|-------|------|
| Sunny | No |
| Sunny | No |
| Overcast | Yes |
| Rain | Yes |
| Rain | Yes |
| Rain | No |

Step 1: Compute parent entropy

Yes=3, No=3 â†’ Entropy = 1

Step 2: Split on Outlook

Sunny â†’ No, No â†’ Entropy=0
Overcast â†’ Yes â†’ Entropy=0
Rain â†’ Yes, Yes, No â†’ Entropy=0.918

Step 3: Weighted entropy

= (2/6)*0 + (1/6)*0 + (3/6)*0.918 = 0.459

Information Gain = 1 âˆ’ 0.459 = 0.541

---

## 8ï¸âƒ£ DECISION TREE AS REGRESSOR

Instead of class labels:
- Leaves store mean value

Splits minimize:
- Variance
- Mean Squared Error

Tree predicts average of leaf.

---

## 9ï¸âƒ£ WHY TREES OVERFIT

Trees keep splitting until:
- Zero impurity
- Memorization

This gives:
- Low bias
- High variance

---

## ðŸ”Ÿ REGULARIZATION IN DECISION TREES

Trees are regularized by **stopping growth**:

- max_depth
- min_samples_split
- min_samples_leaf
- max_features

This is pruning.

---

## 1ï¸âƒ£1ï¸âƒ£ PRE-PRUNING VS POST-PRUNING

- Pre-pruning: stop early
- Post-pruning: grow fully, then cut back

Goal:
> Balance bias and variance

---

## ðŸ§  100 INTERVIEW QUESTIONS (WITH ANSWERS)

### ðŸŸ¢ Basic (1â€“20)
1. What is a decision tree?
â†’ A model that makes decisions via hierarchical splits.
2. What is a leaf node?
â†’ Final prediction node.
3. What is impurity?
â†’ Measure of class mixing.
4. Why is 50â€“50 most impure?
â†’ Maximum uncertainty.
5. What is entropy?
â†’ Measure of uncertainty.
6. Range of entropy?
â†’ 0 to 1.
7. What is Gini?
â†’ Probability of misclassification.
8. Range of Gini?
â†’ 0 to 0.5.
9. What is information gain?
â†’ Reduction in entropy.
10. Why do trees split greedily?
â†’ Global optimization is NP-hard.
11. What is root node?
â†’ First split.
12. What is depth?
â†’ Longest path from root to leaf.
13. Can trees handle categorical data?
â†’ Yes.
14. Can trees handle missing values?
â†’ Yes (with strategies).
15. Are trees scale-sensitive?
â†’ No.
16. Do trees need normalization?
â†’ No.
17. Are trees interpretable?
â†’ Yes.
18. Bias of trees?
â†’ Low.
19. Variance of trees?
â†’ High.
20. Main weakness?
â†’ Overfitting.

---

### ðŸŸ¡ Intermediate (21â€“50)
21. Entropy vs Gini?
â†’ Similar splits, Gini faster.
22. Why Gini used more?
â†’ Computationally cheaper.
23. Can information gain be negative?
â†’ No.
24. Why trees overfit?
â†’ Keep splitting.
25. How to prevent overfitting?
â†’ Pruning.
26. What is min_samples_leaf?
â†’ Minimum samples per leaf.
27. What is max_depth?
â†’ Maximum depth allowed.
28. Why shallow trees generalize better?
â†’ Reduced variance.
29. What is decision tree regressor?
â†’ Predicts mean value.
30. How splits chosen in regression?
â†’ Minimize variance.
31. Why trees unstable?
â†’ Small data change â†’ big structure change.
32. Why trees good baseline?
â†’ Interpretability.
33. Can trees extrapolate?
â†’ No.
34. What happens with noisy data?
â†’ Overfitting.
35. What is greedy splitting?
â†’ Local optimal choice.
36. Why global optimum not found?
â†’ Computationally infeasible.
37. Handling imbalance in trees?
â†’ Class weights.
38. Trees vs linear models?
â†’ Non-linear splits.
39. When to prefer trees?
â†’ Rule-based logic.
40. Feature importance in trees?
â†’ Based on impurity reduction.
41. Can trees handle interactions?
â†’ Yes, naturally.
42. Why entropy uses log?
â†’ Penalize uncertainty.
43. Gini vs misclassification error?
â†’ Gini smoother.
44. Why misclassification error not used?
â†’ Insensitive to changes.
45. What is CART?
â†’ Binary tree algorithm.
46. CART uses which impurity?
â†’ Gini.
47. ID3 uses which?
â†’ Entropy.
48. Can trees handle outliers?
â†’ Yes.
49. Are trees parametric?
â†’ No.
50. Why trees are high variance?
â†’ Data-driven splits.

---

### ðŸ”´ Advanced & Scenario (51â€“100)
51. Why decision trees struggle with linear boundaries?
â†’ Axis-aligned splits.
52. Why ensemble trees?
â†’ Reduce variance.
53. Tree vs Random Forest?
â†’ Averaging trees.
54. Tree vs XGBoost?
â†’ Sequential error correction.
55. Why pruning improves test accuracy?
â†’ Removes noise-fitting splits.
56. Pre vs post pruning tradeoff?
â†’ Bias vs variance.
57. When use tree regressor?
â†’ Non-linear numeric targets.
58. Can trees overfit small data?
â†’ Yes.
59. What happens if max_depth = None?
â†’ Full memorization.
60. How trees handle missing values?
â†’ Surrogate splits.
61. Can trees learn monotonicity?
â†’ No.
62. Why trees are not smooth?
â†’ Piecewise constant predictions.
63. Why Gini = 1 âˆ’ Î£pÂ²?
â†’ Misclassification probability.
64. Why entropy and gini give similar splits?
â†’ Both convex impurity measures.
65. Can trees be regularized via loss?
â†’ Indirectly.
66. Why trees bad for extrapolation?
â†’ Leaf averages.
67. How trees handle categorical splits?
â†’ One-vs-rest or grouping.
68. When trees beat linear models?
â†’ Strong non-linearity.
69. Tree depth vs interpretability?
â†’ Deeper = less interpretable.
70. How trees handle interactions?
â†’ Sequential splits.
71. Why tree splits are axis-aligned?
â†’ Computational simplicity.
72. What is cost-complexity pruning?
â†’ Penalize depth.
73. Can trees be probabilistic?
â†’ Yes (class proportions).
74. Why trees unstable to noise?
â†’ Greedy decisions.
75. Can trees handle multi-output?
â†’ Yes.
76. Why trees popular in industry?
â†’ Explainability.
77. How trees scale with data?
â†’ O(n log n).
78. Trees vs kNN?
â†’ Structured vs lazy.
79. Why not use entropy always?
â†’ Computational cost.
80. What is split criterion bias?
â†’ Favoring many-category features.
81. How to fix split bias?
â†’ Gain ratio.
82. What is gain ratio?
â†’ Normalized information gain.
83. Trees for time series?
â†’ With feature engineering.
84. Why trees memorize?
â†’ Exact thresholds.
85. Trees vs neural nets?
â†’ Interpretability vs flexibility.
86. What is monotonic constraint?
â†’ Feature monotonicity.
87. Why trees used in credit models?
â†’ Regulatory explainability.
88. Can trees learn XOR?
â†’ Yes.
89. Why trees good for tabular data?
â†’ Flexible splits.
90. What is leaf purity?
â†’ Homogeneity.
91. Why stopping early increases bias?
â†’ Less expressive.
92. How to choose max_depth?
â†’ Cross-validation.
93. Tree vs SVM?
â†’ Interpretability vs margin.
94. Can trees be smoothened?
â†’ Ensembles.
95. Why trees poor at ranking?
â†’ Hard splits.
96. Trees vs NB?
â†’ Rule-based vs probabilistic.
97. Trees vs Logistic?
â†’ Non-linear vs linear boundary.
98. Why trees handle mixed data?
â†’ No scaling required.
99. Biggest mistake using trees?
â†’ No pruning.
100. One sentence summary?
â†’ Trees learn decisions by reducing uncertainty step by step.

---

## âœ… DECISION TREE NOTEBOOK COMPLETE

Next logical step:
ðŸ‘‰ **Random Forests & Ensemble Methods**

