# Classical Machine Learning â€” Geometry, Probability & Optimization

> This repository is a **conceptually complete, math-heavy guide to Classical Machine Learning**.
> It is written to build *true understanding* â€” not just usage â€” by unifying:
>
> * **Geometry** (spaces, projections, margins)
> * **Probability** (uncertainty, likelihoods, latent variables)
> * **Optimization** (loss surfaces, convexity, EM, gradients)
>
> Every topic is explained **from first principles**, with intuition layered *after* formal math.

---

## ğŸ¯ Who This Repository Is For

* Data Scientists who want **deep foundations**
* ML Engineers preparing for **serious interviews**
* Researchers who want **clean conceptual structure**
* Non-statistical engineers who want intuition *without losing rigor*

If you are looking for quick recipes, this is **not** the right place.

---

## ğŸ§  How to Study This Repository (Very Important)

This repository is **order-sensitive**.
Each chapter builds concepts used later.

Below is the **recommended study order**, with *why* each block exists.

---

## ğŸ“˜ PART 1 â€” Linear Models & Geometry Foundations

### 1ï¸âƒ£ Linear Regression

ğŸ“‚ `linear-regression/`

**Why first**:

* Introduces vectors, projections, loss surfaces
* Builds geometric intuition for all ML

**You learn**:

* Least squares geometry
* Biasâ€“variance
* Conditioning & ill-posed problems

---

### 2ï¸âƒ£ Logistic Regression & Softmax

ğŸ“‚ `logistic-regression/`

**Why next**:

* Extends linear geometry to probabilities
* Introduces likelihood & cross-entropy

**You learn**:

* Log-odds
* Convex optimization
* Decision boundaries

---

## ğŸ“˜ PART 2 â€” Margin-Based Learning

### 3ï¸âƒ£ Support Vector Machines (SVM)

ğŸ“‚ `svm/`

**Why here**:

* Shifts from probability to pure geometry
* Introduces margins & duality

**You learn**:

* Convex optimization
* Kernel trick
* Capacity control

---

## ğŸ“˜ PART 3 â€” Tree-Based Models & Ensembles

### 4ï¸âƒ£ Decision Trees

ğŸ“‚ `decision-trees/`

**Why now**:

* Breaks linear geometry
* Introduces axis-aligned partitions

---

### 5ï¸âƒ£ Random Forests

ğŸ“‚ `random-forests/`

**Why next**:

* Introduces variance reduction by averaging

---

### 6ï¸âƒ£ Boosting (AdaBoost â†’ GBM)

ğŸ“‚ `boosting/`

**Why next**:

* Introduces bias reduction
* Functional gradient descent

---

### 7ï¸âƒ£ XGBoost, LightGBM & CatBoost

ğŸ“‚ `xgboost/`, `lightgbm-catboost/`

**Why here**:

* Shows industrial-strength boosting
* Explicit regularization & second-order geometry

---

## ğŸ“˜ PART 4 â€” Dimensionality Reduction & Representation

### 8ï¸âƒ£ PCA

ğŸ“‚ `pca/`

**Why now**:

* Introduces eigenvalues & subspaces
* Foundation for kernels & latent variables

---

### 9ï¸âƒ£ LDA, ICA & Kernel PCA

ğŸ“‚ `lda-ica-kernelpca/`

**Why next**:

* Shows different meanings of â€œimportant directionâ€

---

### ğŸ”Ÿ Manifold Learning (t-SNE & UMAP)

ğŸ“‚ `manifold-learning/`

**Why after PCA**:

* Handles nonlinear structure
* Visualization & topology

---

## ğŸ“˜ PART 5 â€” Probabilistic Models & Latent Variables

### 1ï¸âƒ£1ï¸âƒ£ Bayesian Decision Theory & Naive Bayes

ğŸ“‚ `bayesian-methods/`

**Why here**:

* Formalizes uncertainty & optimal decisions

---

### 1ï¸âƒ£2ï¸âƒ£ Gaussian Mixture Models & EM

ğŸ“‚ `bayesian-methods/`

**Why last**:

* Combines probability, geometry, optimization
* Bridge to deep generative models

---

## ğŸ” Optional Reading Orders

### ğŸ”¹ Interview-Oriented Path

1. Linear Regression
2. Logistic Regression
3. SVM
4. Decision Trees
5. Random Forests
6. XGBoost

---

### ğŸ”¹ Probability-First Path

1. Bayesian Decision Theory
2. Naive Bayes
3. GMM & EM
4. PCA / PPCA

---

## ğŸ§© How This Connects to Deep Learning

This repository prepares you for:

* Backpropagation (from gradient boosting)
* VAEs (from PPCA & GMMs)
* Representation learning (from PCA & kernels)

Deep learning **extends**, not replaces, this foundation.

---

## âœ¨ Final Advice

* Do **not rush**
* Re-derive equations on paper
* Visualize geometry wherever possible
* Revisit earlier chapters often

If you truly understand this repository,
**modern ML will feel natural, not magical**.

---

Happy learning ğŸŒ±
