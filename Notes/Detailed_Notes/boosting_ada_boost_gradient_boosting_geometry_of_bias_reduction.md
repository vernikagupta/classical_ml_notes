# Boosting â€” AdaBoost, Gradient Boosting & Geometry of Bias Reduction

> This chapter is **math-complete first**, intuition-rich second.  
> Focus: *why boosting exists*, *what problem it fixes (bias)*, and *how math + geometry explain its behavior*.

---

## 1. Why Random Forest Is Not the End

From the Random Forest chapter:
- RF dramatically reduces **variance**
- RF does **not** aggressively reduce **bias**

When the true decision boundary is:
- Complex
- Curved
- Requires many rectangles

Averaging many trees still leaves **systematic error**.

Boosting addresses:

> **Bias â€” the error due to underfitting**.

---

## 2. Core Idea of Boosting (One Sentence)

> **Train models sequentially so that each new model focuses on the mistakes of the previous ones.**

This is fundamentally different from bagging:
- Bagging â†’ parallel, independent
- Boosting â†’ sequential, dependent

---

## 3. Weak Learners (Formal Definition)

A weak learner is a model that performs:

Error < 0.5  (binary classification)

Decision stumps (depth-1 trees) are typical weak learners:
- Very high bias
- Very low variance

Boosting converts:

> Many weak learners â†’ one strong learner

---

## 4. AdaBoost â€” Formal Setup

Given:
- Training data (xáµ¢, yáµ¢)
- yáµ¢ âˆˆ {âˆ’1, +1}

Initialize weights:

wáµ¢ = 1/n

Train weak learner hâ‚(x).

---

## 5. Weighted Error (Exact Formula)

Weighted error:

Îµâ‚œ = âˆ‘áµ¢ wáµ¢ Â· ğŸ™(hâ‚œ(xáµ¢) â‰  yáµ¢)

Constraint:

Îµâ‚œ < 0.5

---

## 6. Learner Weight (Why This Formula)

AdaBoost assigns:

Î±â‚œ = 1/2 Â· ln((1 âˆ’ Îµâ‚œ)/Îµâ‚œ)

Meaning:
- Better learner â†’ larger Î±
- Worse learner â†’ smaller Î±

This emerges from **loss minimization**, not heuristics.

---

## 7. Weight Update Rule (Derivation Outcome)

Update sample weights:

wáµ¢ â† wáµ¢ Â· exp(âˆ’Î±â‚œ yáµ¢ hâ‚œ(xáµ¢))

Interpretation:
- Misclassified â†’ weight increases
- Correct â†’ weight decreases

Then normalize.

---

## 8. Final AdaBoost Model

Final classifier:

F(x) = sign( âˆ‘â‚œ Î±â‚œ hâ‚œ(x) )

This is an **additive model**.

---

## 9. Loss Function Behind AdaBoost (Very Important)

AdaBoost minimizes **exponential loss**:

L(y, F(x)) = exp(âˆ’yF(x))

Key properties:
- Strongly penalizes confident mistakes
- Emphasizes hard points

---

## 10. Margin Theory (Geometry)

Margin:

máµ¢ = yáµ¢ F(xáµ¢)

Boosting:
- Increases minimum margin
- Even if training error is already zero

This explains:
- Good generalization
- Resistance to overfitting early

---

## 11. Why AdaBoost Works (Bias View)

Each stump:
- Very biased

Sequential correction:
- Reduces systematic error
- Builds complex boundary step-by-step

Boosting fits:

> Residual structure

---

## 12. Geometry of Boosting

Each weak learner adds:
- A small correction in function space

Overall model:

F(x) = Fâ‚€(x) + âˆ‘â‚œ Î±â‚œ hâ‚œ(x)

This is **functional addition**, not parameter averaging.

---

## 13. From AdaBoost to Gradient Boosting

AdaBoost is a special case of:

> **Gradient Descent in Function Space**

Instead of parameters, we optimize:

F(x)

---

## 14. Gradient Boosting â€” Formal Objective

Minimize:

âˆ‘áµ¢ L(yáµ¢, F(xáµ¢))

Iterative update:

Fâ‚œ(x) = Fâ‚œâ‚‹â‚(x) + Î· hâ‚œ(x)

Where hâ‚œ fits:

âˆ’ âˆ‚L/âˆ‚F |_{Fâ‚œâ‚‹â‚}

---

## 15. Squared Loss Example (Regression)

Loss:

L = (y âˆ’ F(x))Â²

Negative gradient:

ráµ¢ = yáµ¢ âˆ’ F(xáµ¢)

So:
- Each tree fits residuals

---

## 16. Logistic Loss Example (Classification)

Loss:

L = log(1 + exp(âˆ’yF(x)))

Gradient:

ráµ¢ = yáµ¢ âˆ’ Ïƒ(F(xáµ¢))

Boosting fits **pseudo-residuals**.

---

## 17. Learning Rate (Shrinkage)

Small Î·:
- Slower learning
- Better generalization

This controls:
- Biasâ€“variance tradeoff

---

## 18. Overfitting in Boosting (When It Happens)

Boosting can overfit when:
- Noisy labels
- Large trees
- Large learning rate

Boosting is **not immune**.

---

## 19. Comparison: Bagging vs Boosting

| Aspect | Bagging | Boosting |
|------|--------|----------|
| Training | Parallel | Sequential |
| Fixes | Variance | Bias |
| Correlation | Reduced by randomness | Not required |
| Geometry | Averaging | Additive correction |

---

## 20. Intuition Recap (For Non-Statistical Readers)

- Bagging asks many independent opinions
- Boosting listens to mistakes and corrects them
- Each step focuses where model is weak

---

## 21. Bridge Forward

Boosting leads to:
- Gradient Boosting Machines (GBM)
- XGBoost, LightGBM, CatBoost

Next chapter:

> **XGBoost â€” Regularization, Second-Order Geometry & Practical Power**

---

*This chapter completes the ensemble foundations: Trees â†’ RF â†’ Boosting.*

