# Linear Regression â€” Geometry, OLS, Conditioning & Optimization

> **GitHub-ready Markdown**
> This document is a clean, end-to-end Linear Regression chapter written with **geometry-first intuition**, full **OLS derivations**, and **optimization insights**.

---

## Table of Contents

1. Linear Regression â€” Basics
2. Types of Relationships
3. Simple Linear Regression
4. Multiple Linear Regression (Matrix Form)
5. Predictions & Residuals
6. Variance Decomposition (SST, SSR, SSE)
7. Least Squares Objective
8. Objective in Vector Form
9. Expanding the Loss
10. OLS Derivation (Normal Equation)
11. Gram Matrix & Geometry
12. Loss Surface in (eta)-Space
13. Ill-Conditioning & Eigenvalues
14. Geometry of OLS (Projection View)
15. Intercept â€” Why It Matters
16. Gradient Descent
17. Mini-batch & SGD
18. Ridge Regression (Geometric Fix)
19. Mooreâ€“Penrose Pseudoinverse
20. PCA as a Stabilization Tool
21. Unified Geometric View

---

## 1. Linear Regression â€” Basics

Linear Regression models the relationship between input features **X** and a continuous target **y**.

The assumption is **linearity**:

y = XÎ² + Îµ

Where:

* X âˆˆ â„â¿Ë£áµˆ (design matrix)
* Î² âˆˆ â„áµˆ (coefficients)
* Îµ = noise

---

## 2. Types of Relationships

* **Positive**: X â†‘ â†’ y â†‘
* **Negative**: X â†‘ â†’ y â†“
* **None**: No systematic pattern

Covariance gives **direction** (scale-dependent).
Correlation gives **strength** (âˆ’1 to 1).

---

## 3. Simple Linear Regression

y = Î²â‚€ + Î²â‚x

* Î²â‚: slope
* Î²â‚€: intercept

---

## 4. Multiple Linear Regression (Matrix Form)

y = XÎ² + Îµ

Where each column of X is a feature direction in data space.

---

## 5. Predictions & Residuals

Prediction:

yÌ‚ = XÎ²

Residual:

e = y âˆ’ yÌ‚

---

## 6. Variance Decomposition

* Total: SST = âˆ‘(yáµ¢ âˆ’ È³)Â²
* Explained: SSR = âˆ‘(yÌ‚áµ¢ âˆ’ È³)Â²
* Unexplained: SSE = âˆ‘(yáµ¢ âˆ’ yÌ‚áµ¢)Â²

SST = SSR + SSE

---

## 7. Least Squares Objective

We minimize squared error:

J(Î²) = âˆ‘(yáµ¢ âˆ’ xáµ¢áµ€Î²)Â²

Why square?

* Avoid sign cancellation
* Convex
* Differentiable

---

## 8. Objective in Vector Form

J(Î²) = (y âˆ’ XÎ²)áµ€(y âˆ’ XÎ²)

---

## 9. Expanding the Loss

J(Î²) = yáµ€y âˆ’ 2Î²áµ€Xáµ€y + Î²áµ€Xáµ€XÎ²

---

## 10. OLS Derivation (Normal Equation)

Take gradient:

âˆ‚J/âˆ‚Î² = âˆ’2Xáµ€y + 2Xáµ€XÎ²

Set to zero:

Xáµ€XÎ² = Xáµ€y

Solution:

Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€y

---

## 11. Gram Matrix & Geometry

Xáµ€X is the **Gram Matrix**.

* Diagonal: feature magnitudes
* Off-diagonal: feature alignment

Dot product:

xáµ¢áµ€xâ±¼ = ||xáµ¢|| ||xâ±¼|| cosÎ¸

---

## 12. Loss Surface in Î²-Space

Loss is quadratic in Î².

* Orthogonal features â†’ circular contours
* Correlated features â†’ elongated ellipses

Shape fully determined by Xáµ€X.

---

## 13. Ill-Conditioning & Eigenvalues

Eigenvalues of Xáµ€X represent curvature strength.

* Large eigenvalue â†’ strong direction
* Small eigenvalue â†’ flat direction

Inverse amplifies weak directions â†’ instability.

---

## 14. Geometry of OLS (Projection View)

OLS projects y onto column space of X.

Residual is orthogonal:

Xáµ€(y âˆ’ XÎ²) = 0

---

## 15. Intercept â€” Why It Matters

1. Provides baseline when X = 0
2. Prevents forced pass through origin
3. Ensures zero-mean residuals

---

## 16. Gradient Descent

Iterative update:

Î²â½áµ—âºÂ¹â¾ = Î²â½áµ—â¾ âˆ’ Î·âˆ‡J(Î²)

Avoids matrix inverse.

---

## 17. Mini-batch & SGD

* GD: full batch
* Mini-batch: k samples
* SGD: 1 sample

Noise averages out over time.

---

## 18. Ridge Regression (Geometric Fix)

Objective:

J(Î²) = (y âˆ’ XÎ²)áµ€(y âˆ’ XÎ²) + Î»Î²áµ€Î²

Solution:

Î²Ì‚ = (Xáµ€X + Î»I)â»Â¹Xáµ€y

Raises curvature of weak directions.

---

## 19. Mooreâ€“Penrose Pseudoinverse

Used when inverse doesnâ€™t exist.

Via SVD:

X = UÎ£Váµ€

Xâº = VÎ£âºUáµ€

Drops zero-information directions.

---

## 20. PCA as a Stabilization Tool

PCA rotates features to orthogonal axes.

* Removes redundancy
* Makes Xáµ€X diagonal
* Hard regularization

---

## 21. Unified Geometric View

| Method        | Effect              |
| ------------- | ------------------- |
| OLS           | Uses all directions |
| Ridge         | Shrinks weak ones   |
| Pseudoinverse | Drops zero-info     |
| PCA + OLS     | Rotate + prune      |

**Everything is geometry.**

---

# Logistic Regression â€” Geometry, Likelihood, Curvature & Optimization

> **Same depth and philosophy as Linear Regression notes**
> Focus: **what changes, why it changes, and how geometry + math explain it**.

---

## 1. Why Linear Regression Fails for Classification

Suppose y âˆˆ {0,1}.

Linear regression predicts:

yÌ‚ = XÎ²

Problems:

* Predictions are unbounded (âˆ’âˆ, +âˆ)
* Errors are not symmetric
* Squared loss assumes Gaussian noise â†’ violated

Classification needs:

* Output âˆˆ (0,1)
* Probabilistic interpretation

---

## 2. Logistic Model â€” What We Change and Why

### Key Idea

We **do not change the linear score**.

We change **how it is interpreted**.

Linear score:

z = XÎ²

Probability model:

P(y=1 | x) = Ïƒ(z)

Where sigmoid:

Ïƒ(z) = 1 / (1 + e^(âˆ’z))

---

## 3. Why Sigmoid (Not Arbitrary Choice)

Sigmoid satisfies three essential requirements:

1. Maps â„ â†’ (0,1)
2. Monotonic (order preserved)
3. Smooth & differentiable

Most important:

**Sigmoid is the inverse of log-odds**.

---

## 4. Log-Odds (Very Important)

Define odds:

odds = P(y=1|x) / P(y=0|x)

Log-odds:

log(odds) = log(p / (1 âˆ’ p))

Logistic regression assumes:

log(p / (1 âˆ’ p)) = XÎ²

This is why logistic regression is **linear in parameters** but **nonlinear in probability**.

---

## 5. Geometry of Logistic Regression (Decision Space)

Decision boundary:

XÎ² = 0

This is:

* Line (2D)
* Plane (3D)
* Hyperplane (higher-D)

So geometry of **decision boundary** is identical to linear regression.

What changes is:

* How confidence grows away from boundary

---

## 6. Why Squared Loss Is Wrong Here

Classification errors are **asymmetric**:

* Predicting 0.01 instead of 0 â†’ small error
* Predicting 0.01 instead of 1 â†’ catastrophic

Squared loss treats both equally â†’ wrong geometry.

---

## 7. Bernoulli Likelihood â€” Correct Noise Model

For binary y:

P(y|x) = p^y (1 âˆ’ p)^(1âˆ’y)

Where:

p = Ïƒ(XÎ²)

Likelihood over dataset:

L(Î²) = âˆáµ¢ Ïƒ(záµ¢)^{yáµ¢} (1 âˆ’ Ïƒ(záµ¢))^{1âˆ’yáµ¢}

---

## 8. Log-Likelihood (Why Log)

Take log:

â„“(Î²) = âˆ‘áµ¢ [ yáµ¢ log Ïƒ(záµ¢) + (1 âˆ’ yáµ¢) log(1 âˆ’ Ïƒ(záµ¢)) ]

Why log:

* Turns product â†’ sum
* Numerically stable
* Convex in Î²

---

## 9. Loss Function â€” Cross Entropy

Negative log-likelihood:

J(Î²) = âˆ’ âˆ‘áµ¢ [ yáµ¢ log Ïƒ(záµ¢) + (1 âˆ’ yáµ¢) log(1 âˆ’ Ïƒ(záµ¢)) ]

This is **cross-entropy loss**.

---

## 10. Gradient â€” Why Optimization Is Iterative

Gradient:

âˆ‡J(Î²) = Xáµ€(Ïƒ(XÎ²) âˆ’ y)

Key difference from OLS:

* Gradient depends on Î² nonlinearly
* No closed-form solution

Hence:

ğŸ‘‰ **Gradient descent is mandatory**.

---

## 11. Geometry of the Loss Surface

Loss surface:

* Convex
* Bowl-shaped
* BUT curvature changes with Î²

Near decision boundary:

* High curvature

Far away:

* Sigmoid saturates
* Gradients become small

This explains:

* Slow convergence
* Importance of feature scaling

---

## 12. Hessian â€” Curvature Explained

Hessian:

H = Xáµ€ W X

Where:

W = diag( Ïƒ(záµ¢)(1 âˆ’ Ïƒ(záµ¢)) )

Interpretation:

* Data-dependent curvature
* Points near boundary contribute most

---

## 13. Ill-Conditioning Reappears (Same Villain)

If features are correlated:

* Xáµ€WX becomes ill-conditioned
* Flat directions appear again

Same geometry as OLS â€” but **weighted by confidence**.

---

## 14. Regularization in Logistic Regression

### L2 (Ridge-style)

J(Î²) = CrossEntropy + Î»Î²áµ€Î²

Effect:

* Shrinks weak directions
* Stabilizes boundary

### L1 (Lasso-style)

* Sparse Î²
* Feature selection

---

## 15. Newtonâ€™s Method vs Gradient Descent

Newton update:

Î²âº = Î² âˆ’ Hâ»Â¹ âˆ‡J

Pros:

* Faster convergence

Cons:

* Computing Hâ»Â¹ expensive

This leads to:

* IRLS
* Quasi-Newton (L-BFGS)

---

## 16. Probabilistic Interpretation (Final Insight)

Logistic regression models:

P(y=1 | x) = Ïƒ(XÎ²)

Not a hard classifier.

Thresholding (e.g. 0.5) is a **post-processing choice**, not part of the model.

---

## 17. Unified View (Linear â†’ Logistic)

| Aspect      | Linear Regression | Logistic Regression  |
| ----------- | ----------------- | -------------------- |
| Output      | â„                 | (0,1)                |
| Noise model | Gaussian          | Bernoulli            |
| Loss        | Squared error     | Cross-entropy        |
| Solution    | Closed form       | Iterative            |
| Geometry    | Projection        | Boundary + curvature |

---

## 18. One Mental Model to Remember

* Linear regression fits **mean**
* Logistic regression fits **log-odds**
* Geometry of XÎ² stays the same
* Loss geometry changes because probability is nonlinear

This is why logistic regression is **not just linear regression + sigmoid**.
